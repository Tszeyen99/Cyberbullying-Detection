{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /Users/tszeyenthen/Python Study/venv-sereinsetupjupyter/lib/python3.11/site-packages (4.66.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emot in /Users/tszeyenthen/Python Study/venv-sereinsetupjupyter/lib/python3.11/site-packages (3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install emot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /Users/tszeyenthen/Python Study/venv-sereinsetupjupyter/lib/python3.11/site-packages (3.7.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/tszeyenthen/Python Study/venv-sereinsetupjupyter/lib/python3.11/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/tszeyenthen/Python Study/venv-sereinsetupjupyter/lib/python3.11/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/tszeyenthen/Python Study/venv-sereinsetupjupyter/lib/python3.11/site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/tszeyenthen/Python Study/venv-sereinsetupjupyter/lib/python3.11/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/tszeyenthen/Python Study/venv-sereinsetupjupyter/lib/python3.11/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /Users/tszeyenthen/Python Study/venv-sereinsetupjupyter/lib/python3.11/site-packages (from spacy) (8.2.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/tszeyenthen/Python Study/venv-sereinsetupjupyter/lib/python3.11/site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/tszeyenthen/Python Study/venv-sereinsetupjupyter/lib/python3.11/site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/tszeyenthen/Python Study/venv-sereinsetupjupyter/lib/python3.11/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /Users/tszeyenthen/Python Study/venv-sereinsetupjupyter/lib/python3.11/site-packages (from spacy) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Users/tszeyenthen/Python Study/venv-sereinsetupjupyter/lib/python3.11/site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/tszeyenthen/Python Study/venv-sereinsetupjupyter/lib/python3.11/site-packages (from spacy) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/tszeyenthen/Python Study/venv-sereinsetupjupyter/lib/python3.11/site-packages (from spacy) (4.66.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/tszeyenthen/Python Study/venv-sereinsetupjupyter/lib/python3.11/site-packages (from spacy) (2.28.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/tszeyenthen/Python Study/venv-sereinsetupjupyter/lib/python3.11/site-packages (from spacy) (2.5.3)\n",
      "Requirement already satisfied: jinja2 in /Users/tszeyenthen/Python Study/venv-sereinsetupjupyter/lib/python3.11/site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /Users/tszeyenthen/Python Study/venv-sereinsetupjupyter/lib/python3.11/site-packages (from spacy) (65.5.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/tszeyenthen/Python Study/venv-sereinsetupjupyter/lib/python3.11/site-packages (from spacy) (23.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/tszeyenthen/Python Study/venv-sereinsetupjupyter/lib/python3.11/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Users/tszeyenthen/Python Study/venv-sereinsetupjupyter/lib/python3.11/site-packages (from spacy) (1.24.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/tszeyenthen/Python Study/venv-sereinsetupjupyter/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in /Users/tszeyenthen/Python Study/venv-sereinsetupjupyter/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.14.6)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/tszeyenthen/Python Study/venv-sereinsetupjupyter/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/tszeyenthen/Python Study/venv-sereinsetupjupyter/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/tszeyenthen/Python Study/venv-sereinsetupjupyter/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/tszeyenthen/Python Study/venv-sereinsetupjupyter/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/tszeyenthen/Python Study/venv-sereinsetupjupyter/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/tszeyenthen/Python Study/venv-sereinsetupjupyter/lib/python3.11/site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/tszeyenthen/Python Study/venv-sereinsetupjupyter/lib/python3.11/site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/tszeyenthen/Python Study/venv-sereinsetupjupyter/lib/python3.11/site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.3)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /Users/tszeyenthen/Python Study/venv-sereinsetupjupyter/lib/python3.11/site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/tszeyenthen/Python Study/venv-sereinsetupjupyter/lib/python3.11/site-packages (from jinja2->spacy) (2.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /Users/tszeyenthen/Python Study/venv-sereinsetupjupyter/lib/python3.11/site-packages (4.3.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /Users/tszeyenthen/Python Study/venv-sereinsetupjupyter/lib/python3.11/site-packages (from gensim) (1.24.2)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /Users/tszeyenthen/Python Study/venv-sereinsetupjupyter/lib/python3.11/site-packages (from gensim) (1.11.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/tszeyenthen/Python Study/venv-sereinsetupjupyter/lib/python3.11/site-packages (from gensim) (6.4.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: language-tool-python in /Users/tszeyenthen/Python Study/venv-sereinsetupjupyter/lib/python3.11/site-packages (2.7.3)\n",
      "Requirement already satisfied: requests in /Users/tszeyenthen/Python Study/venv-sereinsetupjupyter/lib/python3.11/site-packages (from language-tool-python) (2.28.2)\n",
      "Requirement already satisfied: tqdm in /Users/tszeyenthen/Python Study/venv-sereinsetupjupyter/lib/python3.11/site-packages (from language-tool-python) (4.66.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/tszeyenthen/Python Study/venv-sereinsetupjupyter/lib/python3.11/site-packages (from requests->language-tool-python) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/tszeyenthen/Python Study/venv-sereinsetupjupyter/lib/python3.11/site-packages (from requests->language-tool-python) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/tszeyenthen/Python Study/venv-sereinsetupjupyter/lib/python3.11/site-packages (from requests->language-tool-python) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/tszeyenthen/Python Study/venv-sereinsetupjupyter/lib/python3.11/site-packages (from requests->language-tool-python) (2022.12.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install language-tool-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /Users/tszeyenthen/Python Study/venv-sereinsetupjupyter/lib/python3.11/site-packages (from en-core-web-sm==3.7.1) (3.7.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/tszeyenthen/Python Study/venv-sereinsetupjupyter/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/tszeyenthen/Python Study/venv-sereinsetupjupyter/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/tszeyenthen/Python Study/venv-sereinsetupjupyter/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/tszeyenthen/Python Study/venv-sereinsetupjupyter/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/tszeyenthen/Python Study/venv-sereinsetupjupyter/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /Users/tszeyenthen/Python Study/venv-sereinsetupjupyter/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/tszeyenthen/Python Study/venv-sereinsetupjupyter/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/tszeyenthen/Python Study/venv-sereinsetupjupyter/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/tszeyenthen/Python Study/venv-sereinsetupjupyter/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /Users/tszeyenthen/Python Study/venv-sereinsetupjupyter/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Users/tszeyenthen/Python Study/venv-sereinsetupjupyter/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/tszeyenthen/Python Study/venv-sereinsetupjupyter/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/tszeyenthen/Python Study/venv-sereinsetupjupyter/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/tszeyenthen/Python Study/venv-sereinsetupjupyter/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.28.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/tszeyenthen/Python Study/venv-sereinsetupjupyter/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.3)\n",
      "Requirement already satisfied: jinja2 in /Users/tszeyenthen/Python Study/venv-sereinsetupjupyter/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /Users/tszeyenthen/Python Study/venv-sereinsetupjupyter/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (65.5.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/tszeyenthen/Python Study/venv-sereinsetupjupyter/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/tszeyenthen/Python Study/venv-sereinsetupjupyter/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Users/tszeyenthen/Python Study/venv-sereinsetupjupyter/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.24.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/tszeyenthen/Python Study/venv-sereinsetupjupyter/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in /Users/tszeyenthen/Python Study/venv-sereinsetupjupyter/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.14.6)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/tszeyenthen/Python Study/venv-sereinsetupjupyter/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/tszeyenthen/Python Study/venv-sereinsetupjupyter/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/tszeyenthen/Python Study/venv-sereinsetupjupyter/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/tszeyenthen/Python Study/venv-sereinsetupjupyter/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/tszeyenthen/Python Study/venv-sereinsetupjupyter/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2022.12.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/tszeyenthen/Python Study/venv-sereinsetupjupyter/lib/python3.11/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/tszeyenthen/Python Study/venv-sereinsetupjupyter/lib/python3.11/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/tszeyenthen/Python Study/venv-sereinsetupjupyter/lib/python3.11/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.3)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /Users/tszeyenthen/Python Study/venv-sereinsetupjupyter/lib/python3.11/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/tszeyenthen/Python Study/venv-sereinsetupjupyter/lib/python3.11/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.2)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install textblob\n",
    "pip install tqdm\n",
    "pip install emot\n",
    "pip install spacy\n",
    "pip install gensim\n",
    "pip install language-tool-python\n",
    "# !python -m spacy download en\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h2/q_r6y6bd5f5c9hvfsfnt31xr0000gn/T/ipykernel_10037/991846668.py:1: DtypeWarning: Columns (12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('/Users/tszeyenthen/Python Study/jupyter notebbok/Cyberbullying/fyp/amica-cyberbullying-distribute/askfm-cyberbullying-data/concatenated_results1.csv')\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('/Users/tszeyenthen/Python Study/jupyter notebbok/Cyberbullying/fyp/amica-cyberbullying-distribute/askfm-cyberbullying-data/concatenated_results1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns = 'Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label counts after remove duplicates Counter({0: 91836, 1: 5154})\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicates based on the 'text' column\n",
    "df = df.drop_duplicates(subset='text', keep='first')\n",
    "# df = df.drop_duplicates(inplace=True)\n",
    "\n",
    "df\n",
    "label_counts = Counter(df['Cyberbullying'])\n",
    "print(\"Label counts after remove duplicates\", label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(corpus):\n",
    "    '''\n",
    "    Function returns unique words in document corpus\n",
    "    '''\n",
    "    # vocab set\n",
    "    unique_words = set()\n",
    "    \n",
    "    # looping through each document in corpus\n",
    "    for document in tqdm(corpus):\n",
    "        for word in document.split(\" \"):\n",
    "            if len(word) > 2:\n",
    "                unique_words.add(word)\n",
    "    \n",
    "    return unique_words\n",
    "\n",
    "def create_profane_mapping(profane_words,vocabulary):\n",
    "    '''\n",
    "    Function creates a mapping between commonly found profane words and words in \n",
    "    document corpus \n",
    "    '''\n",
    "    \n",
    "    # mapping dictionary\n",
    "    mapping_dict = dict()\n",
    "    \n",
    "    # looping through each profane word\n",
    "    for profane in tqdm(profane_words):\n",
    "        mapped_words = set()\n",
    "        \n",
    "        # looping through each word in vocab\n",
    "        for word in vocabulary:\n",
    "            # mapping only if ratio > 80\n",
    "            try:\n",
    "                if fuzz.ratio(profane,word) > 90:\n",
    "                    mapped_words.add(word)\n",
    "            except:\n",
    "                pass\n",
    "                \n",
    "        # list of all vocab words for given profane word\n",
    "        mapping_dict[profane] = mapped_words\n",
    "    \n",
    "    return mapping_dict\n",
    "def replace_words(corpus,mapping_dict):\n",
    "    '''\n",
    "    Function replaces obfuscated profane words using a mapping dictionary\n",
    "    '''\n",
    "    \n",
    "    processed_corpus = []\n",
    "    \n",
    "    # iterating over each document in the corpus\n",
    "    for document in tqdm(corpus):\n",
    "        \n",
    "        # splitting sentence to word\n",
    "        comment = document.split()\n",
    "        \n",
    "        # iterating over mapping_dict\n",
    "        for mapped_word,v in mapping_dict.items():\n",
    "            \n",
    "            # comparing target word to each comment word \n",
    "            for target_word in v:\n",
    "                \n",
    "                # each word in comment\n",
    "                for i,word in enumerate(comment):\n",
    "                    if word == target_word:\n",
    "                        comment[i] = mapped_word\n",
    "        \n",
    "        # joining comment words\n",
    "        document = \" \".join(comment)\n",
    "        document = document.strip()\n",
    "                    \n",
    "        processed_corpus.append(document)\n",
    "        \n",
    "    return processed_corpus\n",
    "\n",
    "# Functions\n",
    "def get_term_list(path):\n",
    "    '''\n",
    "    Function to import term list file\n",
    "    '''\n",
    "    word_list = []\n",
    "    with open(path,\"r\") as f:\n",
    "        for line in f:\n",
    "            word = line.replace(\"\\n\",\"\").strip()\n",
    "            word_list.append(word)\n",
    "    return word_list\n",
    "\n",
    "term_badword_list = get_term_list(\"/Users/tszeyenthen/Python Study/jupyter notebbok/Cyberbullying/fyp/amica-cyberbullying-distribute/askfm-cyberbullying-data/badwords_list.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label counts before remove duplicates Counter({0: 118172, 1: 5376})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/tszeyenthen/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/tszeyenthen/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/tszeyenthen/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tqdm import tqdm\n",
    "import language_tool_python\n",
    "import pickle  # Importing pickle here\n",
    "import json\n",
    "import re\n",
    "import unicodedata\n",
    "import string\n",
    "import nltk\n",
    "import spacy\n",
    "import pickle\n",
    "from emot.emo_unicode import UNICODE_EMOJI, EMOTICONS_EMO\n",
    "from collections import Counter\n",
    "\n",
    "# Load the English language model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "tool = language_tool_python.LanguageTool('en-US')\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Load your DataFrame\n",
    "# df = pd.read_csv('your_dataset.csv')\n",
    "tqdm.pandas()  # Initialize tqdm for Pandas integration\n",
    "\n",
    "\n",
    "label_counts = Counter(df['Cyberbullying'])\n",
    "print(\"Label counts before remove duplicates\", label_counts)\n",
    "def preprocess_text(df):\n",
    "    \"\"\"Preprocess text data in a DataFrame.\"\"\"\n",
    "    \n",
    "    def _get_ner(x):\n",
    "        return \" \".join([ent.label_ for ent in nlp(x).ents])\n",
    "\n",
    "    def _get_pos_tag(x):\n",
    "        return \" \".join([token.pos_ for token in nlp(x)])\n",
    "\n",
    "    def _remove_urls(x):\n",
    "\n",
    "        return re.sub(r\"\\b(?:http|https|ftp|ssh)://[^\\s]*\", '', x)\n",
    "\n",
    "    def _remove_mention(x):\n",
    "        return re.sub(r\"@\\w+\", '', x)\n",
    "\n",
    "    def _remove_emails(x):\n",
    "        return re.sub(r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b\", \"\", x)\n",
    "\n",
    "    def _remove_space_single_chars(x):\n",
    "        temp = re.sub(r'(?i)(?<=\\b[a-z]) (?=[a-z]\\b)', '', x)\n",
    "        return temp\n",
    "    \n",
    "    def normalize_text(text):\n",
    "        # Handle specific patterns of laughter\n",
    "        text = re.sub(r'\\b(ha)+\\b', 'haha', text, flags=re.IGNORECASE)\n",
    "        text = re.sub(r'\\b(lol)+\\b', 'lol', text, flags=re.IGNORECASE)\n",
    "        text = re.sub(r'\\b(lmao)+\\b', 'lmao', text, flags=re.IGNORECASE)\n",
    "\n",
    "        # Reduce elongated sequences of characters\n",
    "        pattern = re.compile(r\"(.)\\1{2,}\")\n",
    "        text = pattern.sub(lambda match: match.group(1), text)\n",
    "        text = re.sub(r'(.)\\1{2,}', r'\\1', text)\n",
    "        text = re.sub(r'(.)\\1{2,}',r'\\1',text)   #any characters, numbers, symbols\n",
    "        text = re.sub(r'(..)\\1{2,}', r'\\1', text)  \n",
    "        text = re.sub(r'(...)\\1{2,}', r'\\1', text)\n",
    "        text = re.sub(r'(....)\\1{2,}', r'\\1', text)  \n",
    "\n",
    "        return text\n",
    "    \n",
    "    def normalize_text_with_original_casing(text):\n",
    "        # Store original casing of words\n",
    "        original_casing_mapping = {}\n",
    "        \n",
    "        # Find unique words and store their original casing\n",
    "        for word in set(text.split()):\n",
    "            original_casing_mapping[word.lower()] = word\n",
    "        \n",
    "        # Normalize text by converting to lowercase and reducing elongated words\n",
    "        normalized_text = normalize_text(text.lower())\n",
    "        \n",
    "        # Restore original casing using the mapping\n",
    "        restored_text = \" \".join(original_casing_mapping.get(word, word) for word in normalized_text.split())\n",
    "        \n",
    "        return restored_text\n",
    "\n",
    "    # Define the function to replace emoticons with descriptions\n",
    "    def replace_emoticons_with_descriptions(text):\n",
    "    # Load Emoji Dictionary\n",
    "        emoji_dict_path = '/Users/tszeyenthen/Python Study/jupyter notebbok/Cyberbullying/fyp/amica-cyberbullying-distribute/askfm-cyberbullying-data/Emoji_Dict.p'\n",
    "        with open(emoji_dict_path, 'rb') as file:\n",
    "            emoji_dict = pickle.load(file)\n",
    "\n",
    "        # Load the emoticon dictionary from the JSON file\n",
    "        emoticon_dict_path = '/Users/tszeyenthen/Python Study/jupyter notebbok/Cyberbullying/fyp/amica-cyberbullying-distribute/askfm-cyberbullying-data/emoticon_dict.json'\n",
    "        with open(emoticon_dict_path, 'r') as file:\n",
    "            emoticon_dict = json.load(file)\n",
    "        \n",
    "        # Define unwanted characters explicitly\n",
    "        unwanted_chars = \"[£♛™→✔♡†☯♫✌®تح♕★ツ☠♚©♥█║▌│☁☀ღ◄ ▲ ► ▼ ◄ ▲ ► ▼▼ ◄ ▲ ► ▼﻿ ◄ ▲ ► ▼ ◄ ▲ ► ▼ ◄ ▲ ► ▼ ◄﻿ ▲ ▼ ◄ ▲ ► ▼ ◄ ▲ ► ▼ ◄▼﻿ ◄ ▲ ►… — … — ¯¯ … ¯ — … ¯ … ¯ ¯ ¯ … – ¯¯ …… ¯¯ ¯ … ¯ ¯¯ ……¯¯ … ¯ ¯ — … ¯¯– – … ¯ ¯ … ¯ ¯¯ … ¯¯ – … – ¯¯ ¯ — — ¯ ¯¯ – … – ¯¯ — — ¯ … ¯ ¯¯ – ¯¯ … – … —– ¯ …… ¯¯ … ¯ — ¯¯ … … ¯]\"\n",
    "        \n",
    "        # Remove unwanted characters\n",
    "        text = re.sub(unwanted_chars, \" \", text)\n",
    "\n",
    "        # Replace emoticons with descriptions\n",
    "        for emoticon, description in emoticon_dict.items():\n",
    "            text = text.replace(emoticon, description)\n",
    "\n",
    "        return text\n",
    "    \n",
    "    add_emoticon = {'-.-': 'shame',\n",
    "      '-_-': 'squiting',\n",
    "      '^.^': 'happy',\n",
    "      ':0': 'surprise',\n",
    "      '^-^': 'happy',\n",
    "      ':33': 'happy face smiley',\n",
    "      '^__^': 'happy',\n",
    "      '-____-': 'shame',\n",
    "      'o_o': 'confused',\n",
    "      'O_O': 'confused',\n",
    "      'x3': 'Cute',\n",
    "      'T T': 'Cry'\n",
    "      }\n",
    "\n",
    "    EMOTICONS_EMO.update(add_emoticon)\n",
    "\n",
    "    pattern_emoticon = u'|'.join(k.replace('|','\\\\|') for k in EMOTICONS_EMO)\n",
    "    pattern_emoticon = pattern_emoticon.replace('\\\\','\\\\\\\\')\n",
    "    pattern_emoticon = pattern_emoticon.replace('(','\\\\(')\n",
    "    pattern_emoticon = pattern_emoticon.replace(')','\\\\)')\n",
    "    pattern_emoticon = pattern_emoticon.replace('[','\\\\[')\n",
    "    pattern_emoticon = pattern_emoticon.replace(']','\\\\]')\n",
    "    pattern_emoticon = pattern_emoticon.replace('*','\\\\*')\n",
    "    pattern_emoticon = pattern_emoticon.replace('+','\\\\+')\n",
    "    pattern_emoticon = pattern_emoticon.replace('^','\\\\^')\n",
    "    pattern_emoticon = pattern_emoticon.replace('·','\\\\·')\n",
    "    pattern_emoticon = pattern_emoticon.replace('\\{','\\\\{')\n",
    "    pattern_emoticon = pattern_emoticon.replace('\\}','\\\\}')\n",
    "    pattern_emoticon = pattern_emoticon.replace('<','\\\\>')\n",
    "    pattern_emoticon = pattern_emoticon.replace('>','\\\\>')\n",
    "    pattern_emoticon = pattern_emoticon.replace('?','\\\\?')\n",
    "\n",
    "    # Convert emoticons into word\n",
    "    def _convert_emoticons(x):\n",
    "        for emot in EMOTICONS_EMO:\n",
    "            x = x.replace(emot, \"_\".join(EMOTICONS_EMO[emot].replace(\",\",\"\").replace(\":\",\"\").split()))\n",
    "        return x\n",
    "\n",
    "    # Count emoji\n",
    "    pattern_emoji = u'|'.join(k.replace('|','\\\\|') for k in UNICODE_EMOJI)\n",
    "    pattern_emoji = pattern_emoji.replace('\\\\','\\\\\\\\')\n",
    "    pattern_emoji = pattern_emoji.replace('(','\\\\(')\n",
    "    pattern_emoji = pattern_emoji.replace(')','\\\\)')\n",
    "    pattern_emoji = pattern_emoji.replace('[','\\\\[')\n",
    "    pattern_emoji = pattern_emoji.replace(']','\\\\]')\n",
    "    pattern_emoji = pattern_emoji.replace('*','\\\\*')\n",
    "    pattern_emoji = pattern_emoji.replace('+','\\\\+')\n",
    "    pattern_emoji = pattern_emoji.replace('^','\\\\^')\n",
    "    pattern_emoji = pattern_emoji.replace('·','\\\\·')\n",
    "    pattern_emoji = pattern_emoji.replace('\\{','\\\\{·')\n",
    "    pattern_emoji = pattern_emoji.replace('\\}','\\\\}·')\n",
    "\n",
    "\n",
    "    # Convert emoji into word\n",
    "    def _convert_emojis(x):\n",
    "        for emot in UNICODE_EMOJI:\n",
    "            x = x.replace(emot, \"_\".join(UNICODE_EMOJI[emot].replace(\",\",\"\").replace(\":\",\"\").split()))\n",
    "        return x\n",
    "\n",
    "    def remove_accented_chars(x):\n",
    "        x = unicodedata.normalize('NFKD', x).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        return x\n",
    "\n",
    "    def _slang_resolution(x):\n",
    "        slang_path = '/Users/tszeyenthen/Python Study/jupyter notebbok/Cyberbullying/fyp/amica-cyberbullying-distribute/askfm-cyberbullying-data/SLANG_SOCIAL.pkl'\n",
    "        with open(slang_path, 'rb') as fp:\n",
    "            slang_path = pickle.load(fp)\n",
    "        clean_text = []\n",
    "        for text in x.split():\n",
    "            if text in list(slang_path.keys()):\n",
    "                for key in slang_path:\n",
    "                    value = slang_path[key]\n",
    "                    if text == key:\n",
    "                        clean_text.append(text.replace(key,value))\n",
    "                    else:\n",
    "                        continue\n",
    "            else:\n",
    "                clean_text.append(text)\n",
    "        return \" \".join(clean_text)\n",
    "\n",
    "    # Sample function to normalize text with original casing\n",
    "    def slang_resolution__with_original_casing(text):\n",
    "        \n",
    "        # Store original casing of words\n",
    "        original_casing_mapping = {}\n",
    "        \n",
    "        # Find unique words and store their original casing\n",
    "        for word in set(text.split()):\n",
    "            original_casing_mapping[word.lower()] = word\n",
    "        \n",
    "        # Normalize text by converting to lowercase and reducing elongated words\n",
    "        normalized_text = _slang_resolution(text.lower())\n",
    "        \n",
    "        # Restore original casing using the mapping\n",
    "        restored_text = \" \".join(original_casing_mapping.get(word, word) for word in normalized_text.split())\n",
    "        \n",
    "        return restored_text\n",
    "\n",
    "    # Function to expand contractions in text\n",
    "    def expand_contractions(text):\n",
    "        # Define the CONTRACTION_MAP dictionary\n",
    "        CONTRACTION_MAP = {\n",
    "        \"ain't\": \"is not\",\n",
    "        \"aren't\": \"are not\",\n",
    "        \"can't\": \"cannot\",\n",
    "        \"can't've\": \"cannot have\",\n",
    "        \"'cause\": \"because\",\n",
    "        \"could've\": \"could have\",\n",
    "        \"couldn't\": \"could not\",\n",
    "        \"couldn't've\": \"could not have\",\n",
    "        \"didn't\": \"did not\",\n",
    "        \"doesn't\": \"does not\",\n",
    "        \"don't\": \"do not\",\n",
    "        \"hadn't\": \"had not\",\n",
    "        \"hadn't've\": \"had not have\",\n",
    "        \"hasn't\": \"has not\",\n",
    "        \"haven't\": \"have not\",\n",
    "        \"he'd\": \"he would\",\n",
    "        \"he'd've\": \"he would have\",\n",
    "        \"he'll\": \"he will\",\n",
    "        \"he'll've\": \"he he will have\",\n",
    "        \"he's\": \"he is\",\n",
    "        \"how'd\": \"how did\",\n",
    "        \"how'd'y\": \"how do you\",\n",
    "        \"how'll\": \"how will\",\n",
    "        \"how's\": \"how is\",\n",
    "        \"I'd\": \"I would\",\n",
    "        \"I'd've\": \"I would have\",\n",
    "        \"I'll\": \"I will\",\n",
    "        \"I'll've\": \"I will have\",\n",
    "        \"I'm\": \"I am\",\n",
    "        \"I've\": \"I have\",\n",
    "        \"i'd\": \"i would\",\n",
    "        \"i'd've\": \"i would have\",\n",
    "        \"i'll\": \"i will\",\n",
    "        \"i'll've\": \"i will have\",\n",
    "        \"i'm\": \"i am\",\n",
    "        \"i've\": \"i have\",\n",
    "        \"isn't\": \"is not\",\n",
    "        \"it'd\": \"it would\",\n",
    "        \"it'd've\": \"it would have\",\n",
    "        \"it'll\": \"it will\",\n",
    "        \"it'll've\": \"it will have\",\n",
    "        \"it's\": \"it is\",\n",
    "        \"let's\": \"let us\",\n",
    "        \"ma'am\": \"madam\",\n",
    "        \"mayn't\": \"may not\",\n",
    "        \"might've\": \"might have\",\n",
    "        \"mightn't\": \"might not\",\n",
    "        \"mightn't've\": \"might not have\",\n",
    "        \"must've\": \"must have\",\n",
    "        \"mustn't\": \"must not\",\n",
    "        \"mustn't've\": \"must not have\",\n",
    "        \"needn't\": \"need not\",\n",
    "        \"needn't've\": \"need not have\",\n",
    "        \"o'clock\": \"of the clock\",\n",
    "        \"oughtn't\": \"ought not\",\n",
    "        \"oughtn't've\": \"ought not have\",\n",
    "        \"shan't\": \"shall not\",\n",
    "        \"sha'n't\": \"shall not\",\n",
    "        \"shan't've\": \"shall not have\",\n",
    "        \"she'd\": \"she would\",\n",
    "        \"she'd've\": \"she would have\",\n",
    "        \"she'll\": \"she will\",\n",
    "        \"she'll've\": \"she will have\",\n",
    "        \"she's\": \"she is\",\n",
    "        \"should've\": \"should have\",\n",
    "        \"shouldn't\": \"should not\",\n",
    "        \"shouldn't've\": \"should not have\",\n",
    "        \"so've\": \"so have\",\n",
    "        \"so's\": \"so as\",\n",
    "        \"that'd\": \"that would\",\n",
    "        \"that'd've\": \"that would have\",\n",
    "        \"that's\": \"that is\",\n",
    "        \"there'd\": \"there would\",\n",
    "        \"there'd've\": \"there would have\",\n",
    "        \"there's\": \"there is\",\n",
    "        \"they'd\": \"they would\",\n",
    "        \"they'd've\": \"they would have\",\n",
    "        \"they'll\": \"they will\",\n",
    "        \"they'll've\": \"they will have\",\n",
    "        \"they're\": \"they are\",\n",
    "        \"they've\": \"they have\",\n",
    "        \"to've\": \"to have\",\n",
    "        \"wasn't\": \"was not\",\n",
    "        \"we'd\": \"we would\",\n",
    "        \"we'd've\": \"we would have\",\n",
    "        \"we'll\": \"we will\",\n",
    "        \"we'll've\": \"we will have\",\n",
    "        \"we're\": \"we are\",\n",
    "        \"we've\": \"we have\",\n",
    "        \"weren't\": \"were not\",\n",
    "        \"what'll\": \"what will\",\n",
    "        \"what'll've\": \"what will have\",\n",
    "        \"what're\": \"what are\",\n",
    "        \"what's\": \"what is\",\n",
    "        \"what've\": \"what have\",\n",
    "        \"when's\": \"when is\",\n",
    "        \"when've\": \"when have\",\n",
    "        \"where'd\": \"where did\",\n",
    "        \"where's\": \"where is\",\n",
    "        \"where've\": \"where have\",\n",
    "        \"who'll\": \"who will\",\n",
    "        \"who'll've\": \"who will have\",\n",
    "        \"who's\": \"who is\",\n",
    "        \"who've\": \"who have\",\n",
    "        \"why's\": \"why is\",\n",
    "        \"why've\": \"why have\",\n",
    "        \"will've\": \"will have\",\n",
    "        \"won't\": \"will not\",\n",
    "        \"won't've\": \"will not have\",\n",
    "        \"would've\": \"would have\",\n",
    "        \"wouldn't\": \"would not\",\n",
    "        \"wouldn't've\": \"would not have\",\n",
    "        \"y'all\": \"you all\",\n",
    "        \"y'all'd\": \"you all would\",\n",
    "        \"y'all'd've\": \"you all would have\",\n",
    "        \"y'all're\": \"you all are\",\n",
    "        \"y'all've\": \"you all have\",\n",
    "        \"you'd\": \"you would\",\n",
    "        \"you'd've\": \"you would have\",\n",
    "        \"you'll\": \"you will\",\n",
    "        \"you'll've\": \"you will have\",\n",
    "        \"you're\": \"you are\",\n",
    "        \"you've\": \"you have\",\n",
    "        \"stfu\": \"shut the fuck up\",\n",
    "        \"wtf\": \"what the fuck\",\n",
    "        \" u \": \" you \",\n",
    "        \" ur \": \" your \",\n",
    "        \" n \": \" and \",\n",
    "        \" dis \": \" this \",\n",
    "        \"'d\": \" would\",\n",
    "        }\n",
    "        for contraction, expansion in CONTRACTION_MAP.items():\n",
    "            text = text.replace(contraction, expansion)\n",
    "        return text\n",
    "\n",
    "    # Sample function to normalize text with original casing\n",
    "    def expand_contractions_with_original_casing(text):\n",
    "        # Store original casing of words\n",
    "        original_casing_mapping = {}\n",
    "        \n",
    "        # Find unique words and store their original casing\n",
    "        for word in set(text.split()):\n",
    "            original_casing_mapping[word.lower()] = word\n",
    "        \n",
    "        # Normalize text by converting to lowercase and expanding contractions\n",
    "        normalized_text = expand_contractions(text.lower())\n",
    "        \n",
    "        # Restore original casing using the mapping\n",
    "        restored_text = \" \".join(original_casing_mapping.get(word, word) for word in normalized_text.split())\n",
    "        \n",
    "        return restored_text\n",
    "\n",
    "    def _remove_numeric(x):\n",
    "        return ''.join([i for i in x if not i.isdigit()])\n",
    "\n",
    "    def _remove_special_chars(x):\n",
    "        punct = string.punctuation + \"¶“”‘’\" \n",
    "        for p in punct:\n",
    "            x = x.replace(p, \" \")\n",
    "        return x\n",
    "\n",
    "    def lemmatize_word(text):\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        lemmas = [lemmatizer.lemmatize(word) for word in text]\n",
    "        return lemmas\n",
    "    \n",
    "    # Apply preprocessing steps\n",
    "    print('Text Preprocessing: Developing NER tag count')\n",
    "    df['ner_tags'] = df['text'].progress_apply(_get_ner)\n",
    "    print('Text Preprocessing: Developing POS tag count')\n",
    "    df['pos_tags'] = df['text'].progress_apply(_get_pos_tag)\n",
    "    print('Text Preprocessing: Remove urls, user mention, emails')\n",
    "    df['text_check'] = df['text'].progress_apply(lambda x: _remove_urls(x))\n",
    "    df['text_check'] = df['text_check'].progress_apply(lambda x: _remove_mention(x))\n",
    "    df['text_check'] = df['text_check'].progress_apply(lambda x: _remove_emails(x))\n",
    "    print('Text Preprocessing: Remove single characters')\n",
    "    df['text_check'] = df['text_check'].progress_apply(_remove_space_single_chars)\n",
    "    print('Text Preprocessing: Reduce elongated characters')\n",
    "    df['text_check'] = df['text_check'].progress_apply(lambda x: normalize_text_with_original_casing(x))\n",
    "    print('Text Preprocessing: Reduce accented characters')\n",
    "    df['text_check'] = df['text_check'].progress_apply(remove_accented_chars)\n",
    "    print('Text Preprocessing: Expand contraction')\n",
    "    df['text_check'] = df['text_check'].progress_apply(expand_contractions_with_original_casing)\n",
    "    print('Text Preprocessing: Correct abbreviation or slang')\n",
    "    df['text_check'] = df['text_check'].progress_apply(lambda x: slang_resolution__with_original_casing(x))\n",
    "    print('Text Preprocessing: Normalize emoticons')\n",
    "    df['text_check'] = df['text_check'].progress_apply(lambda x: _convert_emojis(x))\n",
    "    df['text_check'] = df['text_check'].progress_apply(lambda x: _convert_emoticons(x))\n",
    "    df['text_check'] = df['text_check'].progress_apply(replace_emoticons_with_descriptions)\n",
    "    print('Text Preprocessing: Lowercase')\n",
    "    df['text_check'] = df['text_check'].str.lower()\n",
    "    print('Text Preprocessing: Replace obfuscated bad term')\n",
    "    # unique words in vocab \n",
    "    unique_words = get_vocab(corpus= df['text_check'])    \n",
    "     # creating mapping dict \n",
    "    mapping_dict = create_profane_mapping(profane_words=term_badword_list,vocabulary=unique_words)\n",
    "    df['text_check'] = replace_words(corpus=df['text_check'], mapping_dict=mapping_dict)\n",
    "    print('Text Preprocessing: Correct spelling')\n",
    "    df['text_check'] = df['text_check'].progress_apply(lambda x: tool.correct(x))\n",
    "    print('Text Preprocessing: Remove numeric character')\n",
    "    df['text_check'] = df['text_check'].progress_apply(_remove_numeric)\n",
    "    print('Text Preprocessing: Remove punctuations')\n",
    "    df['text_check'] = df['text_check'].progress_apply(lambda x: _remove_special_chars(x))\n",
    "    print('Text Preprocessing: Remove multiple spaces')\n",
    "    df['text_check'] = df['text_check'].progress_apply(lambda x: ' '.join(x.split()))\n",
    "    print('Text Preprocessing: Tokenisation')\n",
    "    df[\"tokenize_text\"] = df.progress_apply(lambda row: nltk.word_tokenize(row['text_check'].lower()), axis=1)\n",
    "    print('Text Preprocessing: Lemmatization')\n",
    "    df[\"lemmatized_text\"] = df[\"tokenize_text\"].progress_apply(lemmatize_word)\n",
    "    df['clean_text'] = df['lemmatized_text'].progress_apply(lambda tokens: ' '.join(tokens))\n",
    "    \n",
    "    # Remove empty texts\n",
    "    df = df[~df['clean_text'].isna()]\n",
    "    df = df[df['clean_text'] != '']\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    print('Done')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                           file_name sentence_id  \\\n",
       " 69476  Askfm_conversation_1_main.xml         s.0   \n",
       " 69477  Askfm_conversation_1_main.xml         s.1   \n",
       " 69483  Askfm_conversation_1_main.xml        s.10   \n",
       " 69484  Askfm_conversation_1_main.xml        s.12   \n",
       " 69485  Askfm_conversation_1_main.xml        s.13   \n",
       " \n",
       "                                                     text  Cyberbullying  \\\n",
       " 69476                      ¶ Have you ever been in love?              0   \n",
       " 69477                                   ¶ I am in love..              0   \n",
       " 69483                ¶ My boyfriend. Who are you dating?              0   \n",
       " 69484  ¶ Well your the true bitch no it not a joke wh...              1   \n",
       " 69485  ¶ Are you really doing this right now? Please ...              1   \n",
       " \n",
       "            role  harmful_score words0                                words1  \\\n",
       " 69476       NaN            NaN    NaN                                   NaN   \n",
       " 69477       NaN            NaN    NaN                                   NaN   \n",
       " 69483       NaN            NaN    NaN                                   NaN   \n",
       " 69484  Harasser            1.0      ¶              Well your the true bitch   \n",
       " 69485    Victim            1.0      ¶  Are you really doing this right now?   \n",
       " \n",
       "                   label1                  words2  ... Racism Defamation  \\\n",
       " 69476                NaN                     NaN  ...      0          0   \n",
       " 69477                NaN                     NaN  ...      0          0   \n",
       " 69483                NaN                     NaN  ...      0          0   \n",
       " 69484     General_insult                   bitch  ...      0          0   \n",
       " 69485  Assertive_selfdef  Please leave me alone.  ...      0          0   \n",
       " \n",
       "       Curse_Exclusion Powerless_selfdef General_defense Attacking_relatives  \\\n",
       " 69476               0                 0               0                   0   \n",
       " 69477               0                 0               0                   0   \n",
       " 69483               0                 0               0                   0   \n",
       " 69484               0                 0               0                   0   \n",
       " 69485               0                 0               0                   0   \n",
       " \n",
       "       Good_characteristics Sarcasm Other Encouraging_harasser  \n",
       " 69476                    0       0     0                    0  \n",
       " 69477                    0       0     0                    0  \n",
       " 69483                    0       0     0                    0  \n",
       " 69484                    0       0     0                    0  \n",
       " 69485                    0       0     0                    0  \n",
       " \n",
       " [5 rows x 64 columns],\n",
       " '/Users/tszeyenthen/Python Study/jupyter notebbok/Cyberbullying/fyp/amica-cyberbullying-distribute/askfm-cyberbullying-data/sorted_numeric_concatenated_results.csv')"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Extract numeric part from the 'file_name' and create a new column for sorting\n",
    "df['file_number'] = df['file_name'].apply(lambda x: int(re.search(r'\\d+', x).group()))\n",
    "\n",
    "# Sort the data by the extracted number and 'sentence_id'\n",
    "sorted_data = df.sort_values(by=['file_number', 'sentence_id'])\n",
    "\n",
    "# Drop the temporary sorting column\n",
    "sorted_data.drop('file_number', axis=1, inplace=True)\n",
    "\n",
    "# Save the newly sorted data to a new CSV file\n",
    "sorted_numeric_file_path = '/Users/tszeyenthen/Python Study/jupyter notebbok/Cyberbullying/fyp/amica-cyberbullying-distribute/askfm-cyberbullying-data/sorted_numeric_concatenated_results.csv'\n",
    "sorted_data.to_csv(sorted_numeric_file_path, index=False)\n",
    "\n",
    "# Show the first few rows to verify sorting\n",
    "sorted_data.head(), sorted_numeric_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before removal:\n",
      "(123548, 64)\n",
      "After removal:\n",
      "(114782, 64)\n",
      "Text Preprocessing: Developing NER tag count\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114782/114782 [06:15<00:00, 305.83it/s]\n",
      "/var/folders/h2/q_r6y6bd5f5c9hvfsfnt31xr0000gn/T/ipykernel_10037/4101585095.py:386: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['ner_tags'] = df['text'].progress_apply(_get_ner)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Preprocessing: Developing POS tag count\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114782/114782 [06:04<00:00, 315.07it/s]\n",
      "/var/folders/h2/q_r6y6bd5f5c9hvfsfnt31xr0000gn/T/ipykernel_10037/4101585095.py:388: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['pos_tags'] = df['text'].progress_apply(_get_pos_tag)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Preprocessing: Remove urls, user mention, emails\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114782/114782 [00:00<00:00, 863139.27it/s]\n",
      "/var/folders/h2/q_r6y6bd5f5c9hvfsfnt31xr0000gn/T/ipykernel_10037/4101585095.py:390: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['text_check'] = df['text'].progress_apply(lambda x: _remove_urls(x))\n",
      "100%|██████████| 114782/114782 [00:00<00:00, 1590880.29it/s]\n",
      "/var/folders/h2/q_r6y6bd5f5c9hvfsfnt31xr0000gn/T/ipykernel_10037/4101585095.py:391: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['text_check'] = df['text_check'].progress_apply(lambda x: _remove_mention(x))\n",
      "100%|██████████| 114782/114782 [00:00<00:00, 871157.90it/s]\n",
      "/var/folders/h2/q_r6y6bd5f5c9hvfsfnt31xr0000gn/T/ipykernel_10037/4101585095.py:392: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['text_check'] = df['text_check'].progress_apply(lambda x: _remove_emails(x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Preprocessing: Remove single characters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114782/114782 [00:00<00:00, 762767.47it/s]\n",
      "/var/folders/h2/q_r6y6bd5f5c9hvfsfnt31xr0000gn/T/ipykernel_10037/4101585095.py:394: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['text_check'] = df['text_check'].progress_apply(_remove_space_single_chars)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Preprocessing: Reduce elongated characters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114782/114782 [00:01<00:00, 64156.25it/s]\n",
      "/var/folders/h2/q_r6y6bd5f5c9hvfsfnt31xr0000gn/T/ipykernel_10037/4101585095.py:396: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['text_check'] = df['text_check'].progress_apply(lambda x: normalize_text_with_original_casing(x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Preprocessing: Reduce accented characters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114782/114782 [00:00<00:00, 1575289.75it/s]\n",
      "/var/folders/h2/q_r6y6bd5f5c9hvfsfnt31xr0000gn/T/ipykernel_10037/4101585095.py:398: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['text_check'] = df['text_check'].progress_apply(remove_accented_chars)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Preprocessing: Expand contraction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114782/114782 [00:01<00:00, 68173.72it/s]\n",
      "/var/folders/h2/q_r6y6bd5f5c9hvfsfnt31xr0000gn/T/ipykernel_10037/4101585095.py:400: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['text_check'] = df['text_check'].progress_apply(expand_contractions_with_original_casing)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Preprocessing: Correct abbreviation or slang\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114782/114782 [01:40<00:00, 1143.45it/s]\n",
      "/var/folders/h2/q_r6y6bd5f5c9hvfsfnt31xr0000gn/T/ipykernel_10037/4101585095.py:402: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['text_check'] = df['text_check'].progress_apply(lambda x: slang_resolution__with_original_casing(x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Preprocessing: Normalize emoticons\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114782/114782 [01:04<00:00, 1778.23it/s]\n",
      "/var/folders/h2/q_r6y6bd5f5c9hvfsfnt31xr0000gn/T/ipykernel_10037/4101585095.py:404: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['text_check'] = df['text_check'].progress_apply(lambda x: _convert_emojis(x))\n",
      "100%|██████████| 114782/114782 [00:06<00:00, 18560.47it/s]\n",
      "/var/folders/h2/q_r6y6bd5f5c9hvfsfnt31xr0000gn/T/ipykernel_10037/4101585095.py:405: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['text_check'] = df['text_check'].progress_apply(lambda x: _convert_emoticons(x))\n",
      "100%|██████████| 114782/114782 [00:48<00:00, 2366.66it/s]\n",
      "/var/folders/h2/q_r6y6bd5f5c9hvfsfnt31xr0000gn/T/ipykernel_10037/4101585095.py:406: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['text_check'] = df['text_check'].progress_apply(replace_emoticons_with_descriptions)\n",
      "/var/folders/h2/q_r6y6bd5f5c9hvfsfnt31xr0000gn/T/ipykernel_10037/4101585095.py:408: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['text_check'] = df['text_check'].str.lower()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Preprocessing: Lowercase\n",
      "Text Preprocessing: Replace obfuscated bad term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114782/114782 [00:00<00:00, 983953.22it/s]\n",
      "100%|██████████| 1723/1723 [00:29<00:00, 58.48it/s]\n",
      "100%|██████████| 114782/114782 [00:05<00:00, 20747.31it/s]\n",
      "/var/folders/h2/q_r6y6bd5f5c9hvfsfnt31xr0000gn/T/ipykernel_10037/4101585095.py:414: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['text_check'] = replace_words(corpus=df['text_check'], mapping_dict=mapping_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Preprocessing: Correct spelling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114782/114782 [30:36<00:00, 62.51it/s] \n",
      "/var/folders/h2/q_r6y6bd5f5c9hvfsfnt31xr0000gn/T/ipykernel_10037/4101585095.py:416: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['text_check'] = df['text_check'].progress_apply(lambda x: tool.correct(x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Preprocessing: Remove numeric character\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114782/114782 [00:00<00:00, 416808.24it/s]\n",
      "/var/folders/h2/q_r6y6bd5f5c9hvfsfnt31xr0000gn/T/ipykernel_10037/4101585095.py:418: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['text_check'] = df['text_check'].progress_apply(_remove_numeric)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Preprocessing: Remove punctuations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114782/114782 [00:00<00:00, 724733.93it/s]\n",
      "/var/folders/h2/q_r6y6bd5f5c9hvfsfnt31xr0000gn/T/ipykernel_10037/4101585095.py:420: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['text_check'] = df['text_check'].progress_apply(lambda x: _remove_special_chars(x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Preprocessing: Remove multiple spaces\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114782/114782 [00:00<00:00, 1349713.34it/s]\n",
      "/var/folders/h2/q_r6y6bd5f5c9hvfsfnt31xr0000gn/T/ipykernel_10037/4101585095.py:422: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['text_check'] = df['text_check'].progress_apply(lambda x: ' '.join(x.split()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Preprocessing: Tokenisation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114782/114782 [00:08<00:00, 13317.11it/s]\n",
      "/var/folders/h2/q_r6y6bd5f5c9hvfsfnt31xr0000gn/T/ipykernel_10037/4101585095.py:424: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"tokenize_text\"] = df.progress_apply(lambda row: nltk.word_tokenize(row['text_check'].lower()), axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Preprocessing: Lemmatization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114782/114782 [00:01<00:00, 66998.84it/s]\n",
      "/var/folders/h2/q_r6y6bd5f5c9hvfsfnt31xr0000gn/T/ipykernel_10037/4101585095.py:426: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"lemmatized_text\"] = df[\"tokenize_text\"].progress_apply(lemmatize_word)\n",
      "100%|██████████| 114782/114782 [00:00<00:00, 2141661.89it/s]\n",
      "/var/folders/h2/q_r6y6bd5f5c9hvfsfnt31xr0000gn/T/ipykernel_10037/4101585095.py:427: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['clean_text'] = df['lemmatized_text'].progress_apply(lambda tokens: ' '.join(tokens))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "After processing:\n",
      "(114782, 70)\n",
      "After processing:\n",
      "(114782, 70)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from langdetect import detect, LangDetectException\n",
    "\n",
    "# Function to detect English text\n",
    "def is_english(text):\n",
    "    try:\n",
    "        return detect(text) == 'en'\n",
    "    except LangDetectException:  # Handle exception if text is too short or detection fails\n",
    "        return False\n",
    "    \n",
    "# Display the DataFrame before removal for comparison\n",
    "print(\"Before removal:\")\n",
    "print(sorted_data.shape)\n",
    "\n",
    "# Step 2: Filter out rows where 'label1' is 'Other_Language'\n",
    "sorted_data_files = sorted_data[sorted_data['label1'] != 'Other_language']\n",
    "\n",
    "# Display the DataFrame after removal to verify the changes\n",
    "print(\"After removal:\")\n",
    "print(sorted_data_files.shape)\n",
    "# sorted_data_files.to_csv('en.csv')\n",
    "\n",
    "preprocess_text(sorted_data_files)\n",
    "# after processing\n",
    "print(\"After processing:\")\n",
    "print(sorted_data_files.shape)\n",
    "sorted_data_files.to_csv('en_after_processing.csv')\n",
    "\n",
    "sorted_data_files_clean = sorted_data_files[['text', 'Cyberbullying']]\n",
    "\n",
    "# # Rename the columns to 'text' and 'label'\n",
    "sorted_data_files_clean.columns = ['text', 'label']\n",
    "sorted_data_files_clean.columns = ['text', 'label']\n",
    "sorted_data_files_clean = sorted_data_files.dropna(subset=['text'])\n",
    "sorted_data_files_clean  = sorted_data_files_clean [sorted_data_files_clean ['text'] != '']\n",
    "sorted_data_files_clean  = sorted_data_files_clean [~sorted_data_files_clean ['text'].isna()]\n",
    "sorted_data_files_clean  = sorted_data_files_clean .reset_index(drop=True)\n",
    "print(\"After processing:\")\n",
    "print(sorted_data_files_clean.shape)\n",
    "sorted_data_files_clean.to_csv('en_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After processing:\n",
      "(112481, 2)\n"
     ]
    }
   ],
   "source": [
    "sorted_data_files_clean = sorted_data_files[['clean_text', 'Cyberbullying']]\n",
    "\n",
    "# Rename the columns to 'text' and 'label'\n",
    "sorted_data_files_clean.columns = ['text', 'label']\n",
    "sorted_data_files_clean = sorted_data_files_clean.dropna(subset=['text'])\n",
    "sorted_data_files_clean  = sorted_data_files_clean [sorted_data_files_clean ['text'] != '']\n",
    "sorted_data_files_clean  = sorted_data_files_clean [~sorted_data_files_clean ['text'].isna()]\n",
    "# sorted_data_files_clean  = sorted_data_files_clean .reset_index(drop=True)\n",
    "print(\"After processing:\")\n",
    "print(sorted_data_files_clean.shape)\n",
    "sorted_data_files_clean.to_csv('en_cleaned.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
